{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of reviews\n",
    "reviews = df['text']\n",
    "\n",
    "# Step 1: Combine all reviews into a single text\n",
    "combined_text = ' '.join(reviews)\n",
    "\n",
    "# Step 2: Tokenize the text (convert to lowercase and remove punctuation)\n",
    "tokens = re.findall('\\w+', combined_text.lower())\n",
    "\n",
    "# Step 3: Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Step 4: Count word frequency\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Step 5: Sort and display the results\n",
    "most_common_words = word_freq.most_common()\n",
    "print(\"Word\\t\\tCount\")\n",
    "print(\"-\" * 20)\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}\\t\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words = pd.DataFrame(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words.columns = ['word', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'filtered_tokens' is your list of words after preprocessing\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, colormap ='cividis', max_font_size=110, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of reviews\n",
    "reviews = df[df['rating'] == 1]['text']\n",
    "\n",
    "# Step 1: Combine all reviews into a single text\n",
    "combined_text = ' '.join(reviews)\n",
    "\n",
    "# Step 2: Tokenize the text (convert to lowercase and remove punctuation)\n",
    "tokens = re.findall('\\w+', combined_text.lower())\n",
    "\n",
    "# Step 3: Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Step 4: Count word frequency\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Step 5: Sort and display the results\n",
    "most_common_words = word_freq.most_common()\n",
    "print(\"Word\\t\\tCount\")\n",
    "print(\"-\" * 20)\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}\\t\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, colormap ='cividis', max_font_size=110, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of reviews\n",
    "reviews = df[df['rating'] == 5]['text']\n",
    "\n",
    "# Step 1: Combine all reviews into a single text\n",
    "combined_text = ' '.join(reviews)\n",
    "\n",
    "# Step 2: Tokenize the text (convert to lowercase and remove punctuation)\n",
    "tokens = re.findall('\\w+', combined_text.lower())\n",
    "\n",
    "# Step 3: Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Step 4: Count word frequency\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Step 5: Sort and display the results\n",
    "most_common_words = word_freq.most_common()\n",
    "print(\"Word\\t\\tCount\")\n",
    "print(\"-\" * 20)\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}\\t\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, colormap ='plasma', max_font_size=110, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize a Counter object to keep track of noun frequencies\n",
    "noun_freq = Counter()\n",
    "\n",
    "text = df['text'][0]\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(combined_text)\n",
    "\n",
    "\n",
    "# Process each word and its part-of-speech tag\n",
    "for word, pos in blob.tags:\n",
    "    if pos.startswith('NN'):  # Check if the POS tag is for a noun\n",
    "        noun_freq[word.lower()] += 1  # Increment the count for the noun in lowercase to ensure case-insensitivity\n",
    "\n",
    "# Print the nouns and their frequencies\n",
    "for noun, freq in noun_freq.items():\n",
    "    print(f\"Noun: {noun}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns = pd.DataFrame(noun_freq.items(), columns=['Noun', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "df_nouns.sort_values('Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.read_csv(\"word_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_label.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    1 = food \n",
    "    2 = Price\n",
    "    3 = envoriment\n",
    "    4 = Employee â€™s Service \n",
    "    5 = waiting time\n",
    "\"\"\"\n",
    "\n",
    "columns = ['text','star','Sentiment', 'Food', 'Price', 'Envoriment', 'Service', \"Waiting Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_label_dict = {word: label for word, label in zip(df_label['Noun'], df_label['label'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    arr = [0,0,0,0,0,0]\n",
    "    for word, pos in blob.tags:\n",
    "        if pos.startswith('NN'): \n",
    "            if word in word_label_dict:\n",
    "                arr[int(word_label_dict[word])] = 1\n",
    "    new_data = {\n",
    "        'text':row['text'],\n",
    "        'star':row['rating'],\n",
    "        'Sentiment': sentiment,\n",
    "        'Food': arr[1],\n",
    "        'Price': arr[2],\n",
    "        'Envoriment': arr[3],\n",
    "        'Service': arr[4],\n",
    "        'Waiting Time': arr[5],\n",
    "\n",
    "    }\n",
    "\n",
    "    new_row = pd.DataFrame(new_data, index=[0])\n",
    "    df_stat = pd.concat([df_stat, new_row], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_stat[df_stat['Envoriment'] == 1]\n",
    "df_temp = df_temp.sort_values(by=\"Sentiment\", ascending=False)\n",
    "df_temp.to_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    # Generate or obtain your data - this could be from any source\n",
    "    new_data = {\n",
    "        'Column1': i,\n",
    "        'Column2': i * 2,\n",
    "        'Column3': i * 3,\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame from the new data\n",
    "    new_row = pd.DataFrame(new_data, index=[0])\n",
    "    \n",
    "    # Append the new data to the DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has columns named 'x_column' and 'y_column'.\n",
    "sns.lmplot(x='Sentiment', y='Food', data=df_stat, y_jitter=.02, logistic=True, truncate=False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation_stylesheet =  'https://raw.githubusercontent.com/TheMikeMerrill/teaching/master/seaborn-example/presentation.mplstyle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "\n",
    "with plt.style.context(presentation_stylesheet):\n",
    "  df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "  # Make a custom palette with gendered colors\n",
    "  pal = dict(male=\"#6495ED\", female=\"#F08080\")\n",
    "\n",
    "  # Show the survival probability as a function of age and sex\n",
    "  g = sns.lmplot(x=\"age\", y=\"survived\", col=\"sex\", hue=\"sex\", data=df,\n",
    "                palette=pal, y_jitter=.02, logistic=True, truncate=False)\n",
    "  g.set(xlim=(0, 80), ylim=(-.05, 1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "print(statsmodels.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'x': [0, 1, 2, 3, 4, 5],\n",
    "    'y': [0, 0, 0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# Plot with logistic regression\n",
    "sns.lmplot(x='x', y='y', data=df, logistic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add a constant term to the predictor\n",
    "df['intercept'] = 1\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.Logit(df['y'], df[['intercept', 'x']])\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Generate predictions using the model\n",
    "df['predictions'] = result.predict(df[['intercept', 'x']])\n",
    "\n",
    "# Plot the data and the logistic curve\n",
    "plt.scatter(df['x'], df['y'], color='red')\n",
    "plt.plot(df['x'], df['predictions'], color='blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Sentiment': [0, 1, 2, 3, 4, 5],\n",
    "    'Food': [0, 0, 0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# Since statsmodels doesn't add a constant by default, we add it manually for the intercept\n",
    "df['intercept'] = 1\n",
    "\n",
    "# Define the model\n",
    "logit_model = sm.Logit(df['Food'], df[['intercept', 'Sentiment']])\n",
    "\n",
    "# Fit the model\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Making predictions over a range of values\n",
    "x_values = np.linspace(df['Sentiment'].min(), df['Sentiment'].max(), 300)\n",
    "# Adding the intercept term for each x\n",
    "x_with_intercept = sm.add_constant(x_values, prepend=True)\n",
    "# Predicting probabilities\n",
    "y_predicted = result.predict(x_with_intercept)\n",
    "\n",
    "# Plotting the actual data points\n",
    "plt.scatter(df['Sentiment'], df['Food'], color='black', zorder=20)\n",
    "# Plotting the logistic regression curve\n",
    "plt.plot(x_values, y_predicted, color='blue')\n",
    "\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Probability of Food')\n",
    "plt.title('Logistic Regression Fit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Sentiment'].value_counts())\n",
    "print(df['Food'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Note: Scikit-learn expects a 2D array for the features, so we reshape if necessary\n",
    "X = df_stat['Sentiment'].values.reshape(-1, 1)\n",
    "y = df_stat['Food']\n",
    "\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Create a range of values for plotting\n",
    "x_values = np.linspace(df_stat['Sentiment'].min(), df_stat['Sentiment'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Calculate the predicted probabilities\n",
    "y_predicted = logreg.predict_proba(x_values)[:, 1]\n",
    "\n",
    "# Plotting the data and the logistic regression curve\n",
    "plt.scatter(df_stat['Sentiment'], df_stat['Food'], color='black')\n",
    "plt.plot(x_values, y_predicted, color='blue')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Probability of Food')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it contains 'Sentiment' as a continuous variable\n",
    "# and 'Food' as a binary outcome (0 or 1).\n",
    "\n",
    "# Prepare the features and response variables\n",
    "X = df_stat[['Sentiment']].values  # Features (2D array for scikit-learn)\n",
    "y = df_stat['Food'].values         # Response (1D array)\n",
    "\n",
    "# Create an instance of the LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Generate a sequence of values for prediction\n",
    "x_values = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities\n",
    "y_probs = logreg.predict_proba(x_values)[:, 1]  # Get the probability of the 1 class\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='black', zorder=20)  # Original data points\n",
    "plt.plot(x_values, y_probs, color='blue')  # Logistic Regression curve\n",
    "plt.xlabel('Sentiment (continuous)')\n",
    "plt.ylabel('Probability of Food (binary)')\n",
    "plt.title('Logistic Regression Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.to_csv(\"stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse481",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
